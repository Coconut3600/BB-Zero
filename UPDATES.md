## January 18, 2026 â€” Miniâ€‘Brain Training Test

Before starting longâ€‘term training with the full-size network, I created a miniâ€‘brain version of BBâ€‘Zero to validate the entire training pipeline. A reference project in English checkers showed that a tiny network can learn quickly, so this experiment helps confirm whether my architecture, MCTS loop, and dataset generation are functioning correctly.

The plan is to run this reduced network for 2â€“3 days and monitor all logs and metrics. If it learns, the full-size network will be restored for long-term training. If not, I will inspect the code for structural issues.

---

## January 20, 2026 â€” Miniâ€‘Brain Results, Fixes, and Next Steps

After two days of testing, the reduced â€œminiâ€‘brainâ€ network showed no learning. Comparing it with the reference project AlphaCheckersâ€‘Zero by MadrasLe (https://github.com/MadrasLe/AlphaCheckers-Zero) helped reveal several structural issues in my pipeline. I also credit MadrasLe for two ideas that improved BBâ€‘Zero significantly: the ORIGINâ€‘based policy mapping and the use of a legalâ€‘moves mask, which my program did not originally include.

Unlike English checkers, Spanish checkers is far more complex (long capture chains, strict forced captures, multiple promotion squares, draw rules, full repetition detection, and a deeper legality system). This complexity requires more training data and a larger network.

To compare fairly, I temporarily reduced my network to match the reference project. This exposed several problems, including Qâ€‘value propagation, priors assignment, legality masking, internal state structure, and policy mapping.  All of these issues have now been fixed.

## January 25, 2026 â€” Preparation for Full Training

Because Spanish checkers demands more capacity, I will return to the fullâ€‘size network and train it with more games than the reference project. Before starting full game training, I will run one final test: a kingâ€‘vsâ€‘king endgame training session to verify stability and behavior.

A new folder, `technical_data/`, will store sample games, metrics, logs, technical notes, and images to document BBâ€‘Zeroâ€™s evolution as the training process becomes fully validated.

## January 30, 2026 â€” Preparing BBâ€‘Zero for the Impossible

Today marks a turning point. I ran two controlled endgame experiments to stressâ€‘test the entire engine under real, slow, unforgiving conditions. Even though BBâ€‘Zero runs in pure Python, on a single machine, without the raw speed of a C++ engine, the system held up beautifully. Parallelism, batched neural inference, and carefully optimized listâ€‘based functions do their part. Slowâ€¦ but solid.

Those two endgames were enough to prove that the entire pipeline is healthy: MCTS, the neural network, dataset generation, and training. Even with low simulation counts, BBâ€‘Zero learned real tactical patterns. That gave me the confidence to design a training plan that can survive my hardware limitations.

The ORIGIN idea (inspired by MadrasLe) turned out to be a huge advantage. It gives structure and context to moves â€” something the network alone cannot infer in Spanish checkers. And with a 15 MB convolutional brain, BBâ€‘Zero has the capacity to learn longâ€‘range plans, complex endings, and its own style of play.

The final goal is ambitious: to face Profound, the absolute monster, the â€œStockfishâ€ of Spanish checkers. A handcrafted C++ engine with decades of tuning and raw speed. If BBâ€‘Zero can compete â€” even a little â€” it will be a massive achievement. A true David vs. Goliath moment, built with one computer, homemade datasets, and stubborn scientific determination.

BBâ€‘Zero is ready. Now begins the brutal training phase: months of discipline, consistency, and almost obsessive persistence. No shortcuts, no miracles. Just a slow python program, a 15 MB brain, and the will to challenge Profound, the giant of giants. From this moment on, the long battle beginsâ€¦ the training starts without stopping.

![Profound](https://github.com/Coconut3600/BB-Zero/blob/main/images/Profound.png?raw=true)


## Training Plan (Febâ€“Jun 2026)

**February 2026**  
â€¢ 40 simulations per move  
â€¢ 200,000 selfâ€‘play games  
â€¢ Train every 10,000 games  
â€¢ Goal: build global intuition and basic pattern recognition

**March 2026**  
â€¢ 80 simulations per move  
â€¢ 100,000 selfâ€‘play games  
â€¢ Train every 5,000 games  
â€¢ Goal: refine defensive structures and stabilize play

**April 2026**  
â€¢ 800 simulations per move  
â€¢ 10,000 selfâ€‘play games  
â€¢ Train every 500 games  
â€¢ Goal: tactical precision, deeper calculation, stronger endings

**May 2026**  
â€¢ 1,600 simulations per move  
â€¢ 5,000 selfâ€‘play games  
â€¢ Train every 250 games  
â€¢ Goal: final polish, style consolidation, tournament readiness

**June 2026**  
â€¢ Final evaluation  
â€¢ Full match: **BBâ€‘Zero vs. Profound**  
â€¢ Goal: measure the true strength of the engine after months of training

## February 5, 2026 â€” Training

BBâ€‘Zero has now reached its fourth training cycle. I havenâ€™t tested games against the brains from cycle 2 or cycle 3 yet, but I did play several games against the brain produced in cycle 1.

In the first test that brain beat me, although I was tired and didnâ€™t play well. The next day I tried again, this time with 80 simulations, and managed to get a draw, but it was difficult. Today I faced the same cycleâ€‘1 brain again, also at 80 simulations, trying to play as well as I couldâ€¦ and it beat me again. Iâ€™m sharing the photo and the full game below.

![BB-Zero vs me](https://github.com/Coconut3600/BB-Zero/blob/main/images/bb_zero_vs_me.jpg?raw=true)

I also talked with my consultant Herson, and we might soon play a game on the PlayOK platform to offer some friendly matches with BBâ€‘Zero for fans of the game. His page is an excellent resource on Spanish Checkers: it documents the best programs, computerâ€‘vsâ€‘computer matches, and games against grandmasters: https://damasclasicas.blogspot.com/

**Game played (0â€“1):**  
1.11-15, 21-18; 2.15-19, 22x15; 3.12x19, 23x14; 4.10x19, 27-23; 5.06-10, 23x14; 6.10x19, 31-27; 7.05-10, 27-23; 8.02-05, 23x14; 9.10x19, 25-21; 10.05-10, 28-23; 11.19x28, 32x23; 12.07-11, 23-19; 13.08-12, 26-22; 14.12-16, 21-17; 15.04-08, 30-27; 16.08-12, 29-26; 17.11-15, 27-23; 18.16-20, 23x16x07; 19.03x12, 26-21; 20.12-16, 19x12; 21.01-05, 22-19; 22.09-13, 18x09x02; 23.10-13, 17x10; 24.16-20, 24x15; 0â€“1

### ğŸ—“ï¸ 2026â€‘02â€‘08 â€” Ciclo 4: crisis, aprendizaje y nuevo rumbo

No soy ingeniero en IA; estoy aprendiendo. Tampoco soy buen jugador de damas clÃ¡sicas espaÃ±olas, por eso uso a *Matilde* como referencia. En este ciclo BBâ€‘Zero empezÃ³ a regalar piezas, como si se frustrara al no poder ganarme, y pensÃ© que habÃ­a daÃ±ado el cerebro por sobreentrenamiento.

Para comprobarlo subÃ­ las simulaciones a 300 y lo enfrentÃ© a Matilde. Para mi sorpresa llegÃ³ a un final empatado, pero terminÃ³ perdiendo porque no sabe mantener la dama. Esto confirmÃ³ que el problema real no es el cerebro: **son los finales**. Sin patrones de finales, el motor evalÃºa mal, cae en trampas y necesita millones de partidas para aprender algo que puedo enseÃ±arle directamente.

DecidÃ­ pausar el entrenamiento general y crear un **curriculum de finales**: pequeÃ±as â€œvitaminasâ€ para el cerebro artificial. EntrenarÃ© posiciones clave (manejo de dama, finales de tablas, persecuciones, trampas tÃ­picas) con simetrÃ­as y cambios de color. Luego mezclarÃ© estos datasets con el selfâ€‘play normal y reanudarÃ© el entrenamiento continuo por varios meses.

---

### Estado del motor en la posiciÃ³n crÃ­tica

Current state â€” 100% completed  
Turn: 2  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
Best move: 24-20  
Q = -0.077  
Visits = 5000  
Ï€ = 0.595  
NN value = -0.089  
Visited nodes: 9 / 9  

Topâ€‘5 policy:  
â€¢ 24-20 â†’ 0.595  
â€¢ 9-18 â†’ 0.231  
â€¢ 9-31 â†’ 0.062  
â€¢ 9-13 â†’ 0.06  
â€¢ 9-2 â†’ 0.035  

Nota: 9-31 es correcto; 24-20 pierde.

---

### Partida completa vs Matilde

1.10-13, 21-17; 2.13-18, 22x13; 3.09x18, 26-21; 4.12-15, 21x14;
5.11x18, 29-26; 6.05-10, 23-20; 7.07-11, 28-23; 8.10-14, 17-13;
9.01-05, 13-09; 10.05-10, 27-22; 11.18x27, 31x22; 12.10-13, 26-21;
13.03-07, 21-17; 14.06-10, 09-05; 15.02x09, 20-16; 16.08-12, 25-21;
17.13-18, 22x13x06; 18.14-19, 23x14; 19.11x18x25, 06-02; 20.15-19, 30-27;
21.25-29, 02-05; 22.12-15, 05x23; 23.09-13, 17x10; 24.15-19, 23x14;
25.07-12, 16x07; 26.04x11x18, 10-05; 27.18-21, 27-23; 28.29-12, 23-19;
29.12x22, 05-01; 30.21-26, 01-05; 31.26-30, 05-09; 32.22-08

### ğŸ—“ï¸ 2026â€‘02â€‘08 â€” Cycle 4: crisis, learning, and new direction

Iâ€™m not an AI engineer; Iâ€™m learning as I go. Iâ€™m also not a strong Spanish checkers player, which is why I use *Matilde* (a medium/expert engine) as my reference. In this cycle BBâ€‘Zero started giving away pieces, almost like it was getting frustrated when it couldnâ€™t beat me. I thought I had damaged the neural network with overtraining.

To verify, I increased simulations to 300 and tested BBâ€‘Zero against Matilde. Surprisingly, it reached a drawn endgame, but eventually lost because it doesnâ€™t know how to maintain the king in technical endings. This confirmed the real issue: the brain isnâ€™t broken â€” **it simply doesnâ€™t understand endgames**.

Without endgame patterns, the engine:
- mis-evaluates positions  
- falls into traps  
- fails to convert winning positions  
- and would need millions of selfâ€‘play games to learn these concepts naturally

So Iâ€™m pausing general training and creating a **curriculum of endgames** â€” small â€œvitaminsâ€ for the artificial brain. Iâ€™ll train key endings (king handling, draw techniques, traps, races) with symmetry and color inversion. After generating these datasets, Iâ€™ll mix them with normal selfâ€‘play and resume longâ€‘term training for several months.

---

### Engine state in the critical position

Current state â€” 100% completed  
Turn: 2  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
Best move: 24-20  
Q = -0.077  
Visits = 5000  
Ï€ = 0.595  
NN value = -0.089  
Visited nodes: 9 / 9  

Topâ€‘5 policy:  
â€¢ 24-20 â†’ 0.595  
â€¢ 9-18 â†’ 0.231  
â€¢ 9-31 â†’ 0.062  
â€¢ 9-13 â†’ 0.06  
â€¢ 9-2 â†’ 0.035  

Note: 9-31 is correct; 24-20 loses.

---

### Full game vs Matilde

1.10-13, 21-17; 2.13-18, 22x13; 3.09x18, 26-21; 4.12-15, 21x14;
5.11x18, 29-26; 6.05-10, 23-20; 7.07-11, 28-23; 8.10-14, 17-13;
9.01-05, 13-09; 10.05-10, 27-22; 11.18x27, 31x22; 12.10-13, 26-21;
13.03-07, 21-17; 14.06-10, 09-05; 15.02x09, 20-16; 16.08-12, 25-21;
17.13-18, 22x13x06; 18.14-19, 23x14; 19.11x18x25, 06-02; 20.15-19, 30-27;
21.25-29, 02-05; 22.12-15, 05x23; 23.09-13, 17x10; 24.15-19, 23x14;
25.07-12, 16x07; 26.04x11x18, 10-05; 27.18-21, 27-23; 28.29-12, 23-19;
29.12x22, 05-01; 30.21-26, 01-05; 31.26-30, 05-09; 32.22-08




